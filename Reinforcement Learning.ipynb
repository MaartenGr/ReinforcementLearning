{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by installing the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym\n",
    "!pip install procgen\n",
    "!pip install gym-retro\n",
    "!pip install stable-baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gym**  \n",
    "The gym environment is typically used when creating and leveraging new environments/games to train agents on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The observation space of the environment is Box(4,)\n",
      "The action space of the environment is Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "print(f\"The observation space of the environment is {env.observation_space}\")\n",
    "print(f\"The action space of the environment is {env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retro**  \n",
    "The retro environments are fun to use as you can take any ROM and train an agent on that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The observation space of the environment is Box(224, 320, 3)\n",
      "The action space of the environment is MultiBinary(12)\n"
     ]
    }
   ],
   "source": [
    "import retro\n",
    "env = retro.make(game='Airstriker-Genesis')\n",
    "print(f\"The observation space of the environment is {env.observation_space}\")\n",
    "print(f\"The action space of the environment is {env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The observation space of the environment is an image and can therefore best use a Convolution-based neural network policy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Procgen**  \n",
    "These environments are great as they can be trained very fast and require very little overhead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The observation space of the environment is Box(64, 64, 3)\n",
      "The action space of the environment is Discrete(15)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "param = {\"num_levels\": 1, \"distribution_mode\": \"hard\"}\n",
    "env = gym.make(\"procgen:procgen-leaper-v0\", **param)\n",
    "print(f\"The observation space of the environment is {env.observation_space}\")\n",
    "print(f\"The action space of the environment is {env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, note that the observation space is an image and policies should be adapted accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train PPO\n",
    "Now, it is a simple manner of selecting the correct environment and corresponding algorithm and policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import PPO2\n",
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "model = PPO2(MlpPolicy, env, verbose=1)\n",
    "model.learn(total_timesteps=50_000, log_interval=10)\n",
    "\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that is a very simple example and training agents in more difficult environments typically takes tens of millions, sometimes hundreds of millions steps. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
